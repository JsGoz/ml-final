{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966cba04-ef1a-4ddd-bc98-4ab26eae8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # reemplaza 4 por la cantidad de cores que quieras usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ec02da-5ac7-4b2d-966d-9697fac791be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Carga\n",
    "# =========================\n",
    "parquet_std = r\"C:\\PROY_FINAL_ML\\data\\processed\\cba_clean_std.parquet\"\n",
    "parquet_clean = r\"C:\\PROY_FINAL_ML\\data\\processed\\cba_clean.parquet\"\n",
    "\n",
    "path_in = parquet_std if os.path.exists(parquet_std) else parquet_clean\n",
    "df = pd.read_parquet(path_in)\n",
    "\n",
    "# Asegurar nombres esperados\n",
    "col_producto = \"producto_std\" if \"producto_std\" in df.columns else \"producto\"\n",
    "col_present  = \"presentacion\" if \"presentacion\" in df.columns else (\"medida\" if \"medida\" in df.columns else None)\n",
    "col_cadena   = \"cadena\" if \"cadena\" in df.columns else None\n",
    "\n",
    "# Tipos\n",
    "# Asegurar que la columna es tipo datetime mensual\n",
    "df[\"anio_mes\"] = pd.to_datetime(df[\"anio_mes\"])\n",
    "df = df.sort_values([\"anio_mes\", col_producto, \"establecimiento\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2707d8b-6190-4910-87e0-878326584e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Ingeniería de características\n",
    "# =========================\n",
    "# Trabajamos por serie (producto + establecimiento)\n",
    "grp_keys = [col_producto, \"establecimiento\"]\n",
    "\n",
    "def add_time_features(g):\n",
    "    g = g.sort_values(\"anio_mes\").copy()\n",
    "    # Lags\n",
    "    for L in [1, 3, 6]:\n",
    "        g[f\"lag_{L}\"] = g[\"precio\"].shift(L)\n",
    "    # Moving Averages y Volatilidades\n",
    "    g[\"MA_3\"]  = g[\"precio\"].rolling(window=3, min_periods=1).mean()\n",
    "    g[\"MA_6\"]  = g[\"precio\"].rolling(window=6, min_periods=1).mean()\n",
    "    g[\"VOL_3\"] = g[\"precio\"].rolling(window=3, min_periods=2).std()\n",
    "    g[\"VOL_6\"] = g[\"precio\"].rolling(window=6, min_periods=2).std()\n",
    "    # Objetivo de regresión: precio del próximo mes\n",
    "    g[\"y_next\"] = g[\"precio\"].shift(-1)\n",
    "    # % cambio al próximo mes y alerta (clasificación)\n",
    "    g[\"pct_change_next\"] = (g[\"y_next\"] - g[\"precio\"]) / g[\"precio\"]\n",
    "    g[\"alert_next\"] = (g[\"pct_change_next\"] >= 0.05).astype(\"Int64\")\n",
    "    return g\n",
    "\n",
    "df = df.groupby(grp_keys, group_keys=False).apply(add_time_features)\n",
    "\n",
    "# Mediana mensual por producto (across establecimientos)\n",
    "med_mensual = (\n",
    "    df.groupby([col_producto, \"anio_mes\"])[\"precio\"]\n",
    "      .median()\n",
    "      .rename(\"mediana_prod_mes\")\n",
    "      .reset_index()\n",
    ")\n",
    "df = df.merge(med_mensual, on=[col_producto, \"anio_mes\"], how=\"left\")\n",
    "df[\"ratio_precio_mediana\"] = df[\"precio\"] / df[\"mediana_prod_mes\"]\n",
    "\n",
    "# Variables temporales\n",
    "df[\"year\"] = df[\"anio_mes\"].dt.year\n",
    "df[\"month\"] = df[\"anio_mes\"].dt.month\n",
    "\n",
    "# Quitamos filas sin objetivo (último mes de cada serie no tiene y_next)\n",
    "df_model = df.dropna(subset=[\"y_next\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a672d3b-8284-4ec3-89d3-b2e179c80b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cortes temporales:\n",
      "Train hasta: 2021-10-01 00:00:00\n",
      "Valid: 2021-11-01 00:00:00 → 2022-01-01 00:00:00\n",
      "Test desde: 2022-02-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Conjuntos Train / Valid / Test por tiempo (estricto)\n",
    "# =========================\n",
    "# Cortes temporales (ajústalos según tu rango real)\n",
    "# Reglas: train < valid < test\n",
    "dates_sorted = np.sort(df_model[\"anio_mes\"].unique())\n",
    "if len(dates_sorted) < 6:\n",
    "    # Con pocos meses, hacemos 50% / 25% / 25%\n",
    "    n = len(dates_sorted)\n",
    "    cut1 = dates_sorted[int(n*0.5)]\n",
    "    cut2 = dates_sorted[int(n*0.75)]\n",
    "else:\n",
    "    # 60% / 20% / 20%\n",
    "    n = len(dates_sorted)\n",
    "    cut1 = dates_sorted[int(n*0.6)]\n",
    "    cut2 = dates_sorted[int(n*0.8)]\n",
    "\n",
    "train = df_model[df_model[\"anio_mes\"] < cut1]\n",
    "valid = df_model[(df_model[\"anio_mes\"] >= cut1) & (df_model[\"anio_mes\"] < cut2)]\n",
    "test  = df_model[df_model[\"anio_mes\"] >= cut2]\n",
    "\n",
    "print(\"Cortes temporales:\")\n",
    "print(\"Train hasta:\", train[\"anio_mes\"].max())\n",
    "print(\"Valid:\", valid[\"anio_mes\"].min(), \"→\", valid[\"anio_mes\"].max())\n",
    "print(\"Test desde:\", test[\"anio_mes\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216ae2fc-1cf2-4b32-bb7c-d8614b131694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Baseline LV (lag 1) → RMSE: 0.2440 | MAE: 0.0690\n",
      "[TRAIN] Baseline Estacional lag 3 → RMSE: 0.3678 | MAE: 0.1378\n",
      "[TRAIN] Baseline Estacional lag 6 → RMSE: 0.4325 | MAE: 0.1724\n",
      "[VALID] Baseline LV (lag 1) → RMSE: 0.2629 | MAE: 0.0695\n",
      "[VALID] Baseline Estacional lag 3 → RMSE: 0.3875 | MAE: 0.1330\n",
      "[VALID] Baseline Estacional lag 6 → RMSE: 0.4243 | MAE: 0.1615\n",
      "[TEST] Baseline LV (lag 1) → RMSE: 0.2529 | MAE: 0.0636\n",
      "[TEST] Baseline Estacional lag 3 → RMSE: 0.4220 | MAE: 0.1483\n",
      "[TEST] Baseline Estacional lag 6 → RMSE: 0.4651 | MAE: 0.1718\n",
      "Rango TRAIN: 2021-01-01 00:00:00 a 2021-10-01 00:00:00\n",
      "Cantidad meses únicos en TRAIN: 10\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Baselines (lag 1, 3 y 6)sfsfdsdfgashdfa\n",
    "# =========================\n",
    "# Crear lags si no existen\n",
    "for L in [3, 6]:\n",
    "    col = f\"lag_{L}\"\n",
    "    if col not in df_model.columns:\n",
    "        df_model[col] = df_model.groupby(grp_keys)[\"precio\"].shift(L)\n",
    "\n",
    "# Crear copias para evitar SettingWithCopyWarning\n",
    "train_copy = train.copy()\n",
    "valid_copy = valid.copy()\n",
    "test_copy  = test.copy()\n",
    "\n",
    "# Asignar lags a cada subconjunto\n",
    "for L in [3, 6]:\n",
    "    col = f\"lag_{L}\"\n",
    "    train_copy[col] = df_model.loc[train_copy.index, col]\n",
    "    valid_copy[col] = df_model.loc[valid_copy.index, col]\n",
    "    test_copy[col]  = df_model.loc[test_copy.index, col]\n",
    "\n",
    "# Funciones métricas seguras\n",
    "def safe_rmse(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2)))\n",
    "\n",
    "def safe_mae(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    return float(np.mean(np.abs(y_true[mask] - y_pred[mask])))\n",
    "\n",
    "# Evaluar baselines con lag 1 (naive), lag 3 y lag 6\n",
    "for part_name, part in [(\"TRAIN\", train_copy), (\"VALID\", valid_copy), (\"TEST\", test_copy)]:\n",
    "    y_true = part[\"y_next\"].values\n",
    "\n",
    "    # Baseline naive (lag 1)\n",
    "    y_hat_lv = part[\"precio\"].values\n",
    "    print(f\"[{part_name}] Baseline LV (lag 1) → RMSE: {safe_rmse(y_true, y_hat_lv):.4f} | MAE: {safe_mae(y_true, y_hat_lv):.4f}\")\n",
    "\n",
    "    # Baseline estacionales lag 3 y 6\n",
    "    for L in [3, 6]:\n",
    "        col = f\"lag_{L}\"\n",
    "        if part[col].notna().any():\n",
    "            mask = part[col].notna()\n",
    "            y_hat = part.loc[mask, col].values\n",
    "            y_true_masked = part.loc[mask, \"y_next\"].values\n",
    "            print(f\"[{part_name}] Baseline Estacional lag {L} → RMSE: {safe_rmse(y_true_masked, y_hat):.4f} | MAE: {safe_mae(y_true_masked, y_hat):.4f}\")\n",
    "        else:\n",
    "            print(f\"[{part_name}] Baseline Estacional lag {L} → no disponible\")\n",
    "\n",
    "print(\"Rango TRAIN:\", train[\"anio_mes\"].min(), \"a\", train[\"anio_mes\"].max())\n",
    "print(\"Cantidad meses únicos en TRAIN:\", train[\"anio_mes\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989db21d-41f0-490a-b53a-3ae41910195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Preparación de features y pipelines scikit-learn\n",
    "# =========================\n",
    "# Features numéricas y categóricas\n",
    "num_feats = [\"precio\", \"lag_1\", \"lag_3\", \"lag_6\", \"MA_3\", \"MA_6\", \"VOL_3\", \"VOL_6\",\n",
    "             \"ratio_precio_mediana\", \"mediana_prod_mes\", \"year\", \"month\"]\n",
    "cat_feats = [col_producto, \"establecimiento\"]\n",
    "if col_present is not None: cat_feats.append(col_present)\n",
    "if col_cadena is not None:  cat_feats.append(col_cadena)\n",
    "\n",
    "# ColumnTransformer\n",
    "preproc_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\", StandardScaler())\n",
    "        ]), num_feats),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ]), cat_feats)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preproc_clf = preproc_reg  # mismo preprocesamiento\n",
    "\n",
    "# Datasets finales\n",
    "X_train, y_train = train[num_feats + cat_feats], train[\"y_next\"]\n",
    "X_valid, y_valid = valid[num_feats + cat_feats], valid[\"y_next\"]\n",
    "X_test,  y_test  = test[num_feats + cat_feats],  test[\"y_next\"]\n",
    "\n",
    "# Para clasificación (alerta)\n",
    "y_train_cls = train[\"alert_next\"].fillna(0).astype(int)\n",
    "y_valid_cls = valid[\"alert_next\"].fillna(0).astype(int)\n",
    "y_test_cls  = test[\"alert_next\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0a69fd-afc0-47cd-9373-afea8f52e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[REG:Ridge] VALID → RMSE:0.6233 | MAE:0.2383 | R2:0.927\n",
      "[REG:Ridge] TEST  → RMSE:0.9126 | MAE:0.4252 | R2:0.839\n",
      "\n",
      "[REG:HGBReg] VALID → RMSE:0.2881 | MAE:0.1025 | R2:0.984\n",
      "[REG:HGBReg] TEST  → RMSE:0.7383 | MAE:0.2370 | R2:0.894\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Modelos de REGRESIÓN\n",
    "# =========================\n",
    "models_reg = {\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "#    \"RFReg\": RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, max_depth=None)\n",
    "    \"HGBReg\": HistGradientBoostingRegressor(max_iter=300, random_state=42)\n",
    "}\n",
    "\n",
    "for name, base in models_reg.items():\n",
    "    pipe = Pipeline(steps=[(\"pre\", preproc_reg), (\"model\", base)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred_val = pipe.predict(X_valid)\n",
    "    pred_tst = pipe.predict(X_test)\n",
    "    print(f\"\\n[REG:{name}] VALID → RMSE:{np.sqrt(mean_squared_error(y_valid, pred_val)):.4f} | MAE:{mean_absolute_error(y_valid, pred_val):.4f} | R2:{r2_score(y_valid, pred_val):.3f}\")\n",
    "    print(f\"[REG:{name}] TEST  → RMSE:{np.sqrt(mean_squared_error(y_test, pred_tst)):.4f} | MAE:{mean_absolute_error(y_test, pred_tst):.4f} | R2:{r2_score(y_test, pred_tst):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21aca6f3-d1be-4a49-b878-407d66a64e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLF:LogReg] VALID → Acc:0.759 | F1:0.375 | ROC-AUC:0.837\n",
      "[CLF:LogReg] TEST  → Acc:0.772 | F1:0.317 | ROC-AUC:0.808\n",
      "\n",
      "[CLF:HGBCls] VALID → Acc:0.914 | F1:0.402 | ROC-AUC:0.912\n",
      "[CLF:HGBCls] TEST  → Acc:0.928 | F1:0.379 | ROC-AUC:0.897\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Modelos de CLASIFICACIÓN (alerta)\n",
    "# =========================\n",
    "models_clf = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=200, n_jobs=None, class_weight=\"balanced\"),\n",
    "#    \"RFCls\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1, class_weight=\"balanced\")\n",
    "    \"HGBCls\": HistGradientBoostingClassifier(max_iter=300, random_state=42)    \n",
    "}\n",
    "\n",
    "for name, base in models_clf.items():\n",
    "    pipe = Pipeline(steps=[(\"pre\", preproc_clf), (\"model\", base)])\n",
    "    pipe.fit(X_train, y_train_cls)\n",
    "    p_val = pipe.predict(X_valid)\n",
    "    p_tst = pipe.predict(X_test)\n",
    "    proba_val = pipe.predict_proba(X_valid)[:,1] if hasattr(pipe[\"model\"], \"predict_proba\") else p_val\n",
    "    proba_tst = pipe.predict_proba(X_test)[:,1]  if hasattr(pipe[\"model\"], \"predict_proba\")  else p_tst\n",
    "    print(f\"\\n[CLF:{name}] VALID → Acc:{accuracy_score(y_valid_cls, p_val):.3f} | F1:{f1_score(y_valid_cls, p_val):.3f} | ROC-AUC:{roc_auc_score(y_valid_cls, proba_val):.3f}\")\n",
    "    print(f\"[CLF:{name}] TEST  → Acc:{accuracy_score(y_test_cls, p_tst):.3f} | F1:{f1_score(y_test_cls, p_tst):.3f} | ROC-AUC:{roc_auc_score(y_test_cls, proba_tst):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8440a9af-fdc7-4a46-aa3f-cc3cb59cac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda de hiperparámetros para Ridge con validación temporal...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Mejor alpha: 100.0\n",
      "Mejor score CV (neg MSE): -0.1869884310312663\n",
      "[RIDGE] VALID RMSE: 0.4396 | MAE: 0.2075\n",
      "[RIDGE] TEST  RMSE: 0.8030 | MAE: 0.3348\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7) Hyperparameter tuning con validación temporal para Ridge\n",
    "# =========================\n",
    "# Definimos funciones métricas consistentes\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "# Definimos TimeSeriesSplit (ajusta n_splits si quieres)\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Pipeline para Ridge con preprocesamiento (ya definido)\n",
    "pipe_ridge = Pipeline([\n",
    "    (\"pre\", preproc_reg),\n",
    "    (\"model\", Ridge())\n",
    "])\n",
    "\n",
    "# Grid de parámetros a probar\n",
    "param_grid_ridge = {\n",
    "    \"model__alpha\": [0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV con validación temporal y métrica RMSE (neg MSE para sklearn)\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    pipe_ridge,\n",
    "    param_grid=param_grid_ridge,\n",
    "    cv=tscv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Ridge con validación temporal...\")\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor alpha:\", grid_search_ridge.best_params_[\"model__alpha\"])\n",
    "print(\"Mejor score CV (neg MSE):\", grid_search_ridge.best_score_)\n",
    "\n",
    "# Evaluación en validación y test con mejor modelo\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "\n",
    "y_valid_pred = best_ridge.predict(X_valid)\n",
    "y_test_pred = best_ridge.predict(X_test)\n",
    "\n",
    "print(f\"[RIDGE] VALID RMSE: {rmse(y_valid, y_valid_pred):.4f} | MAE: {mae(y_valid, y_valid_pred):.4f}\")\n",
    "print(f\"[RIDGE] TEST  RMSE: {rmse(y_test, y_test_pred):.4f} | MAE: {mae(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3990d6-edaa-4cc5-9bc0-b9c6771d8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda de hiperparámetros para HistGradientBoostingRegressor con validación temporal...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Mejores parámetros: {'model__learning_rate': 0.1, 'model__max_depth': 5, 'model__min_samples_leaf': 50}\n",
      "Mejor score CV (neg MSE): -0.12521218083560745\n",
      "[HGB] VALID → RMSE: 0.2687 | MAE: 0.1009\n",
      "[HGB] TEST  → RMSE: 0.7374 | MAE: 0.2372\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8) Hyperparameter tuning con validación temporal para HistGradientBoostingRegressor\n",
    "# =========================\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Pipeline con preprocesamiento\n",
    "pipe_hgb = Pipeline([\n",
    "    (\"pre\", preproc_reg),\n",
    "    (\"model\", HistGradientBoostingRegressor(random_state=42, max_iter=200))\n",
    "])\n",
    "\n",
    "# Grid de hiperparámetros (más pequeño para acelerar)\n",
    "param_grid_hgb = {\n",
    "    \"model__max_depth\": [3, 5, None],\n",
    "    \"model__min_samples_leaf\": [20, 50],\n",
    "    \"model__learning_rate\": [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# GridSearchCV con validación temporal\n",
    "grid_search_hgb = GridSearchCV(\n",
    "    pipe_hgb,\n",
    "    param_grid=param_grid_hgb,\n",
    "    cv=tscv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para HistGradientBoostingRegressor con validación temporal...\")\n",
    "grid_search_hgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_search_hgb.best_params_)\n",
    "print(\"Mejor score CV (neg MSE):\", grid_search_hgb.best_score_)\n",
    "\n",
    "# Evaluación en validación y test con mejor modelo\n",
    "best_hgb = grid_search_hgb.best_estimator_\n",
    "\n",
    "y_valid_pred_hgb = best_hgb.predict(X_valid)\n",
    "y_test_pred_hgb  = best_hgb.predict(X_test)\n",
    "\n",
    "print(f\"[HGB] VALID → RMSE: {rmse(y_valid, y_valid_pred_hgb):.4f} | MAE: {mae(y_valid, y_valid_pred_hgb):.4f}\")\n",
    "print(f\"[HGB] TEST  → RMSE: {rmse(y_test, y_test_pred_hgb):.4f} | MAE: {mae(y_test, y_test_pred_hgb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294cbaec-cd92-4b24-b21c-601df2c0a173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets y features guardados en 'data/processed'.\n",
      "✅ Todos los modelos y preprocesadores se han guardado correctamente en 'models'.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Guardar modelos y preprocesadores\n",
    "# =========================\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Modelos de regresión\n",
    "# =========================\n",
    "joblib.dump(best_ridge, \"C:/PROY_FINAL_ML/models/best_ridge.pkl\")\n",
    "joblib.dump(best_hgb, \"C:/PROY_FINAL_ML/models/best_hgb.pkl\")\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Modelos de clasificación\n",
    "# =========================\n",
    "# El último pipeline entrenado en tu loop para HGBCls\n",
    "joblib.dump(pipe, \"C:/PROY_FINAL_ML/models/best_hgb_clf.pkl\")\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Preprocesadores\n",
    "# =========================\n",
    "joblib.dump(preproc_reg, \"C:/PROY_FINAL_ML/models/preproc_reg.pkl\")\n",
    "joblib.dump(preproc_clf, \"C:/PROY_FINAL_ML/models/preproc_clf.pkl\")\n",
    "\n",
    "# =========================\n",
    "# Conjuntos de datos\n",
    "# =========================\n",
    "joblib.dump(X_train, \"C:/PROY_FINAL_ML/data/processed/X_train.pkl\")\n",
    "joblib.dump(X_valid, \"C:/PROY_FINAL_ML/data/processed/X_valid.pkl\")\n",
    "joblib.dump(X_test,  \"C:/PROY_FINAL_ML/data/processed/X_test.pkl\")\n",
    "\n",
    "joblib.dump(y_train, \"C:/PROY_FINAL_ML/data/processed/y_train.pkl\")\n",
    "joblib.dump(y_valid, \"C:/PROY_FINAL_ML/data/processed/y_valid.pkl\")\n",
    "joblib.dump(y_test,  \"C:/PROY_FINAL_ML/data/processed/y_test.pkl\")\n",
    "\n",
    "# =========================\n",
    "# Clasificación\n",
    "# =========================\n",
    "joblib.dump(y_train_cls, \"C:/PROY_FINAL_ML/data/processed/y_train_cls.pkl\")\n",
    "joblib.dump(y_valid_cls, \"C:/PROY_FINAL_ML/data/processed/y_valid_cls.pkl\")\n",
    "joblib.dump(y_test_cls,  \"C:/PROY_FINAL_ML/data/processed/y_test_cls.pkl\")\n",
    "\n",
    "# =========================\n",
    "# Guardar lista de features\n",
    "# =========================\n",
    "num_feats = [\"precio\", \"lag_1\", \"lag_3\", \"lag_6\", \"MA_3\", \"MA_6\", \"VOL_3\", \"VOL_6\",\n",
    "             \"ratio_precio_mediana\", \"mediana_prod_mes\", \"year\", \"month\"]\n",
    "\n",
    "cat_feats = [\"producto_std\", \"establecimiento\", \"presentacion\", \"cadena\"]\n",
    "\n",
    "joblib.dump(num_feats, \"C:/PROY_FINAL_ML/data/processed/num_feats.pkl\")\n",
    "joblib.dump(cat_feats, \"C:/PROY_FINAL_ML/data/processed/cat_feats.pkl\")\n",
    "\n",
    "print(\"✅ Datasets y features guardados en 'data/processed'.\")\n",
    "\n",
    "\n",
    "print(\"✅ Todos los modelos y preprocesadores se han guardado correctamente en 'models'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c89762-987c-4455-903f-f902b220f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
